---
description: 
globs: 
alwaysApply: false
---
# Tiny Supervised Fine-Tuning (SFT)

## Goal
Turn a raw base model into a polite helper.

---

## 1. Dataset

Author 20–30 prompt/response pairs that include:

- 3 factual Q&A examples  
  (e.g., "Capital of France?")

- 3 polite-tone examples  
  (e.g., "Please translate …")

- 2 short-form vs long-form answers  
  (to demonstrate length control)

- 2 "refusal" cases  
  (e.g., illicit request → safe denial)

Wrap each pair using `<|user|>` / `<|assistant|>` tokens.

---

## 2. Model & Training

- Choose: `NousResearch/Meta-Llama-3-8B` or smaller
- Fine-tune using LoRA with PEFT
- Training: 3–5 epochs
- Learning Rate: ≈ `5e-5`

---

## 3. Evaluation

Ask the same 5 prompts before and after fine-tuning.  
Present side-by-side comparisons of responses.

---

## 4. Deliverables

```
q1_sft/
├── dataset.jsonl
├── train.py
├── before_after.md    # screenshots or text
└── README.md

